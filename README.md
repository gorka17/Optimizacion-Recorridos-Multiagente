# Optimizaci√≥n de recorridos con Aprendizaje por Refuerzo Multiagente (MARL)

La optimizaci√≥n de recorridos es un problema clave en sectores como la log√≠stica, el transporte, la planificaci√≥n urbana o la distribuci√≥n de recursos. Resolverlo de manera eficiente no solo reduce costes y tiempos, sino que tambi√©n mejora la utilizaci√≥n de recursos y la calidad del servicio.

Tradicionalmente, este tipo de problemas se han abordado mediante algoritmos de optimizaci√≥n cl√°sica (heur√≠sticas, algoritmos gen√©ticos, etc.). Sin embargo, el **Aprendizaje por Refuerzo Multiagente (MARL)** ofrece un enfoque prometedor: permite que varios agentes aprendan en un entorno compartido, desarrollando estrategias colaborativas o competitivas para alcanzar objetivos individuales y globales.

Este proyecto explora el potencial de los sistemas MARL para resolver problemas de optimizaci√≥n de recorridos. Se ha utilizado como base el repositorio [MultiAgentSB3](https://github.com/inakivazquez/MultiAgentSB3), que proporciona entornos multiagente compatibles con **Stable-Baselines3 (SB3)** y facilita la implementaci√≥n y entrenamiento de agentes en escenarios multiagente de manera similar a los de agente √∫nico.

---

## Uso de SB3 en un entorno multiagente

Aunque *Stable-Baselines3 (SB3)* est√° dise√±ado para entornos de agente √∫nico, este proyecto consigue aplicarlo en escenarios multiagente gracias a una **ejecuci√≥n s√≠ncrona y alternada de los entrenamientos**. Esto es posible por varios motivos:

- Los entornos multiagente pueden estar compuestos por agentes **homog√©neos o heterog√©neos**.  
- Cada agente se entrena de forma independiente con un *wrapper* que lo hace parecer un entorno de agente √∫nico para SB3.  
- Es posible usar **diferentes algoritmos simult√°neamente** en el mismo entorno (ejemplo: entrenar un agente con PPO y otro con SAC).  
- Los agentes pueden tener **espacios de observaci√≥n y acci√≥n distintos**, seg√∫n sus necesidades.  

### ¬øC√≥mo funciona la sincronizaci√≥n?
- El entrenamiento se realiza **alternando fases de aprendizaje** entre los distintos agentes.  
- Mientras un agente se est√° entrenando y actualizando su pol√≠tica, los dem√°s act√∫an con sus pol√≠ticas congeladas (est√°ticas).  
- Este ciclo de alternancia puede configurarse en n√∫mero de iteraciones y timesteps (ejemplo: 3 agentes, 100 iteraciones, 1.000.000 pasos ‚Üí cada agente entrena 10.000 pasos por iteraci√≥n).  
- Tambi√©n es posible **congelar algunos agentes** y continuar entrenando a otro, para analizar c√≥mo mejora en un entorno estacionario.  

### Clases clave
- **`BaseMAEnv`:** clase base de la que heredan los entornos multiagente.  
- **`AgentMAEnv`:** un *wrapper* que convierte a cada agente en un entorno Gymnasium de agente √∫nico, creando la ilusi√≥n de que SB3 entrena solo a un agente. Internamente, existen tantas instancias de esta clase como agentes a entrenar en el entorno.  

En resumen, gracias a este mecanismo, SB3 puede aplicarse directamente a entornos multiagente sin modificar su n√∫cleo, aprovechando su potencia y estabilidad en el entrenamiento de pol√≠ticas.

---

## Puntos clave del proyecto
- **Dise√±o de entornos multiagente:** modelado de problemas de optimizaci√≥n como escenarios MARL.  
- **Entrenamiento con Stable-Baselines3:** uso de algoritmos de Deep RL adaptados a m√∫ltiples agentes.  
- **Aprendizaje colaborativo y competitivo:** an√°lisis de c√≥mo los agentes coordinan sus acciones para mejorar el rendimiento global.   

---

## Lo que he aprendido
Este proyecto me ha permitido:  
- Diferenciar claramente entre entornos de **agente √∫nico** y **multiagente**.  
- Aprender a **modelar problemas reales** como entornos de RL, definiendo estados, acciones y recompensas.  
- Utilizar frameworks modernos como **Stable-Baselines3** y extensiones para MARL.  
- Comprender el papel de la **cooperaci√≥n y coordinaci√≥n** en la optimizaci√≥n de sistemas multiagente.  
- Evaluar de manera cr√≠tica las **ventajas y limitaciones del MARL** frente a enfoques tradicionales de optimizaci√≥n.  

---

## üìÇ Organizaci√≥n del repositorio
- **`demo_1.py` ‚Ä¶ `demo_7.py`:** scripts que muestran *renders* de los recorridos realizados por los agentes en diferentes escenarios y configuraciones.  
- **`base.py` y `utils.py`:** m√≥dulos que contienen elementos clave en la creaci√≥n de entornos y en la definici√≥n del comportamiento de los agentes.  
- **`train.py`:** scripts responsables de lanzar los entrenamientos de los agentes en los distintos entornos.
- **`env_logistic_v0.py` ‚Ä¶ `env_logistic_v2.py`**: Entornos multiagente creados desde la versi√≥n inicial m√°s sencilla hasta la versi√≥n final.

---

## Recursos
- Repositorio base: [MultiAgentSB3](https://github.com/inakivazquez/MultiAgentSB3)  
- Frameworks utilizados: Stable-Baselines3 (SB3), Gym  

---

## Ejemplo del aprendizaje de tres agentes colaborando
![Ruta optimizada](Images/ejemplo_ruta.png)
